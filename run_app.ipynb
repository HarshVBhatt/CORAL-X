{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKKqbxsmoqbw"
      },
      "source": [
        "# Notebook details\n",
        "- Code to write streamlit app and host it using local tunnel\n",
        "- NOTE: code MUST be run on google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxv1D-8oL5a-",
        "outputId": "e30d2e41-d7dd-4fc6-cd0b-84ae176ed6b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#Required libraries to install\n",
        "!pip install streamlit pyngrok transformers datasets evaluate bitsandbytes pinecone-client langchain langchain-community langchain-pinecone -qU pinecone-notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gkPMEgRt8dv",
        "outputId": "a9c6e72e-3d0e-426a-ea07-4a55ac4f05d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No_EyKm5Orn3",
        "outputId": "a7766c69-11f7-49f9-eacb-144bf7aa2f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zX_GvXYoT2S"
      },
      "source": [
        "# App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhC6P88TdKQg",
        "outputId": "1cf2b61f-ae82-48f4-b33c-4f78086879c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import joblib\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import warnings\n",
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeEmbeddings\n",
        "# from langchain_community.vectorstores import Pinecone as PineconeStore\n",
        "from langchain.vectorstores import Pinecone as PineconeStore\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "prediction_pipeline = joblib.load(\"/content/drive/MyDrive/Capstone/artifacts/xgb_pipeline.pkl\")\n",
        "explainer = joblib.load(\"/content/drive/MyDrive/Capstone/artifacts/xgb_shap_explainer.pkl\")\n",
        "X_test = pd.read_csv(\"/content/drive/MyDrive/Capstone/data/test_data_transformed.csv\")\n",
        "\n",
        "# Tokens\n",
        "pinecone_token = \"pinecone_token\"\n",
        "huggingface_token = \"huggingface_token\"\n",
        "\n",
        "st.title(\"CORAL-X\")\n",
        "\n",
        "idx = st.number_input(\n",
        "    \"Select a record index from X_test\",\n",
        "    min_value=0,\n",
        "    max_value=len(X_test) - 1,\n",
        "    value=0\n",
        ")\n",
        "\n",
        "if st.button(\"Predict and Explain\"):\n",
        "    single_row = X_test.iloc[[idx]]\n",
        "\n",
        "    #Predict outcome and probability\n",
        "    prediction_proba = prediction_pipeline.predict_proba(single_row)[0][1]\n",
        "    prediction_class = prediction_pipeline.predict(single_row)[0]\n",
        "\n",
        "    loan_status_str = \"Loan Rejected\" if prediction_class == 0 else \"Loan Approved\"\n",
        "    st.write(f\"**Model Predicted:** {loan_status_str}\")\n",
        "    st.write(f\"**Probability of Full Payment:** {prediction_proba:.4f}\")\n",
        "\n",
        "    #SHAP computation\n",
        "    preprocessor = prediction_pipeline.named_steps[\"preprocessor\"]\n",
        "    single_row_transformed = preprocessor.transform(single_row)\n",
        "\n",
        "    if hasattr(single_row_transformed, \"toarray\"):\n",
        "        single_row_transformed = single_row_transformed.toarray()\n",
        "\n",
        "    single_shap_values = explainer.shap_values(single_row_transformed)[0]\n",
        "\n",
        "    try:\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "        rename_features = []\n",
        "        for name in feature_names:\n",
        "            rename_features.append(name[5:])\n",
        "    except AttributeError:\n",
        "        rename_features = [f\"{i}\" for i in range(single_row.shape[1])]\n",
        "\n",
        "    #Single DataFrame that includes the SHAP values, Probability, Target\n",
        "    shap_dict = dict(zip(rename_features, single_shap_values))\n",
        "    shap_dict[\"Prediction Proba\"] = prediction_proba\n",
        "    shap_dict[\"Target\"] = prediction_class\n",
        "\n",
        "    shap_row_df = pd.DataFrame([shap_dict])\n",
        "\n",
        "    df_row = single_row.reset_index(drop=True).head(1)\n",
        "    st.write(\"**Record Row:**\", df_row)\n",
        "\n",
        "    ##Prepare prompt for LLM\n",
        "\n",
        "    #Convert state abbrevs\n",
        "\n",
        "    us_state_to_abbrev = {\"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\", \"California\": \"CA\", \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Delaware\": \"DE\",\n",
        "          \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Hawaii\": \"HI\", \"Idaho\": \"ID\", \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\", \"Kansas\": \"KS\", \"Kentucky\": \"KY\",\n",
        "          \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\", \"Massachusetts\": \"MA\", \"Michigan\": \"MI\", \"Minnesota\": \"MN\", \"Mississippi\": \"MS\", \"Missouri\": \"MO\",\n",
        "          \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\", \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\", \"New York\": \"NY\", \"North Carolina\": \"NC\",\n",
        "          \"North Dakota\": \"ND\", \"Ohio\": \"OH\", \"Oklahoma\": \"OK\", \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\", \"South Dakota\": \"SD\",\n",
        "          \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\", \"Vermont\": \"VT\", \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\", \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\",\n",
        "          \"District of Columbia\": \"DC\", \"American Samoa\": \"AS\", \"Guam\": \"GU\", \"Northern Mariana Islands\": \"MP\", \"Puerto Rico\": \"PR\",\n",
        "          \"United States Minor Outlying Islands\": \"UM\", \"Virgin Islands, U.S.\": \"VI\",\n",
        "      }\n",
        "    abbrev_to_us_state = dict(map(reversed, us_state_to_abbrev.items()))\n",
        "\n",
        "    def get_addr_state(idx = 0):\n",
        "      addr_state = abbrev_to_us_state.get(single_row[\"addr_state\"].values[0])\n",
        "      return addr_state\n",
        "\n",
        "    #Data dictionary\n",
        "    data_dict = {\"loan_amnt\": \"Loan amount applied for by borrower\",\n",
        "            \"int_rate\": \"Estimated interest rate on loan\",\n",
        "            \"installment\": \"Monthly installments owed\",\n",
        "            \"grade\": \"Estimated loan grade\",\n",
        "            \"sub_grade\": \"Estimated loan sub-grade\",\n",
        "            \"emp_length\": \"Employment length\",\n",
        "            \"home_ownership\": \"Home ownership status\",\n",
        "            \"annual_inc\": \"Annual income of borrower\",\n",
        "            \"verification_status\": \"Income verification status\",\n",
        "            \"purpose\": \"purpose of loan provided by borrower\",\n",
        "            \"title\": \"Loan title provided by the borrower\",\n",
        "            \"addr_state\": \"Residence state of borrower\",\n",
        "            \"dti\": \"Monthly debt-to-income ratio\",\n",
        "            \"open_acc\": \"Number of borrower's open credit lines\",\n",
        "            \"pub_rec\": \"Number of derogatory public records\",\n",
        "            \"revol_bal\": \"Total credit revolving balance\",\n",
        "            \"revol_util\": \"Revolving line utilization rate\",\n",
        "            \"total_acc\": \"Total number of borrower's credit lines\",\n",
        "            \"initial_list_status\": \"Initial listing status of loan\",\n",
        "            \"application_type\": \"Individual or joint application\",\n",
        "            \"mort_acc\": \"Number of mortgage accounts\",\n",
        "            \"pub_rec_bankruptcies\": \"Number of public record bankruptices\",\n",
        "            \"term_months\": \"Number of monthly payments in loan\",\n",
        "            \"earliest_cr_line_year\": \"Year earliest credit line was opened\",\n",
        "            \"earliest_cr_line_month\": \"Month earliest credit line was opened\"\n",
        "    }\n",
        "\n",
        "    # Convert SHAPs to strings\n",
        "    def get_features_shaps(shap_row, idx = 0):\n",
        "      shap_row_df = shap_row.drop(columns = [\"Prediction Proba\", \"Target\"])\n",
        "\n",
        "      shap_dict = {\"Feature\": [], \"SHAP value\": []}\n",
        "      feature_shap_vals = []\n",
        "\n",
        "      for col in single_row:\n",
        "        if col in shap_row_df:\n",
        "          feature_shap_vals.append(f\"feature_name: {col}, feature_value: {single_row[col].values[0]}, shap_value: {shap_row_df[col].values[0]}\\n\")\n",
        "          shap_dict[\"Feature\"].append(col)\n",
        "          shap_dict[\"SHAP value\"].append(shap_row_df[col].values[0])\n",
        "        else:\n",
        "          try:\n",
        "            col_val = single_row[col].values[0]\n",
        "            shap_val = shap_row_df[f\"{col}_{col_val}\"].values[0]\n",
        "            if col == \"addr_state\":\n",
        "              col_val = abbrev_to_us_state.get(col_val)\n",
        "            feature_shap_vals.append(f\"feature_name: {col}, feature_value: {col_val}, shap_value: {shap_val}\\n\")\n",
        "            shap_dict[\"Feature\"].append(col)\n",
        "            shap_dict[\"SHAP value\"].append(shap_val)\n",
        "          except Exception as e:\n",
        "            raise e\n",
        "\n",
        "\n",
        "      return \"\\n\".join(feature_shap_vals), pd.DataFrame(shap_dict)\n",
        "\n",
        "\n",
        "    ##Pinecone setup\n",
        "    pc = Pinecone(api_key=pinecone_token)\n",
        "    try:\n",
        "      index_name = \"coral-x\"\n",
        "      host = pc.describe_index(index_name)[\"host\"]\n",
        "      index = pc.Index(host = host)\n",
        "      model_name = 'multilingual-e5-large'\n",
        "      embeddings = PineconeEmbeddings(model=model_name, pinecone_api_key=pinecone_token)\n",
        "      # vectordb = PineconeStore(index, embeddings, text_key = \"text\")\n",
        "    except Exception as e:\n",
        "      raise e\n",
        "\n",
        "    ## Download model and tokenizer with 4bit config\n",
        "    compute_dtype = getattr(torch, \"float16\")\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False)\n",
        "\n",
        "    base_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        quantization_config=quant_config,\n",
        "        device_map={\"\": 0},\n",
        "        use_auth_token=huggingface_token\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model,\n",
        "        trust_remote_code=True,\n",
        "        token=huggingface_token,\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Retrieve context + helper functions\n",
        "    def rm_duplicate_lines(text, sep = \"\\n\"):\n",
        "      lines = text.split(sep)\n",
        "      lines_set = set()\n",
        "      clean_text = \"\"\n",
        "      for line in lines:\n",
        "        if line not in lines_set:\n",
        "          # clean_text += \"\\n\".join(line)\n",
        "          clean_text += line + \"\\n\"\n",
        "          lines_set.add(line)\n",
        "\n",
        "      return clean_text\n",
        "\n",
        "    def format_docs(results, clean = False):\n",
        "      rel_docs = results[\"matches\"]\n",
        "      full_doc = \"\"\n",
        "      for doc in rel_docs:\n",
        "        full_doc += \"\".join(doc[\"metadata\"][\"text\"])\n",
        "        full_doc += \"\\n\"\n",
        "\n",
        "      if clean == True:\n",
        "        clean_docs = rm_duplicate_lines(text = full_doc)\n",
        "        return clean_docs\n",
        "\n",
        "      return full_doc\n",
        "\n",
        "    def retrieve(query, namespace, embeddings = embeddings, top_k = 3, pc_index = index, clean = False):\n",
        "      results = index.query(\n",
        "          namespace = namespace,\n",
        "          vector = embeddings.embed_query(query),\n",
        "          top_k = top_k,\n",
        "          include_metadata = True,\n",
        "          include_values = False\n",
        "      )\n",
        "\n",
        "      return format_docs(results, clean = clean)\n",
        "\n",
        "    def generate_context(idx = 0):\n",
        "      query,_ = get_features_shaps(shap_row = shap_row_df, idx = idx)\n",
        "      context = retrieve(\n",
        "          query = query,\n",
        "          namespace = \"data-context-docs\",\n",
        "          top_k = 5,\n",
        "          clean = True)\n",
        "\n",
        "      state = get_addr_state(idx = idx)\n",
        "\n",
        "      context += retrieve(\n",
        "          query = f\"What are the statistics for the state of {state}\",\n",
        "          namespace = \"data-context-docs\",\n",
        "          top_k = 1,\n",
        "          clean = True)\n",
        "\n",
        "      doc_set = set()\n",
        "      for key,val in data_dict.items():\n",
        "        try:\n",
        "          doc = retrieve(\n",
        "              query = f\"How does {val} affect loan eligibility?\",\n",
        "              namespace = \"global-context-docs\",\n",
        "              top_k = 1,\n",
        "              clean = True\n",
        "          )\n",
        "\n",
        "          if doc not in doc_set:\n",
        "            context += doc\n",
        "            doc_set.add(doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "      return context\n",
        "\n",
        "    # Prompt constructor\n",
        "    def custom_prompt(context, idx = 0):\n",
        "\n",
        "      features_shaps,_ = get_features_shaps(shap_row = shap_row_df, idx = idx)\n",
        "      ## Task Description\n",
        "      system_prompt = f\"\"\"You are an assistant for a loan agent who is analyzing a borrower's loan application, \\\n",
        "    The agent has used an XGBoost machine learning model to predict probability of that borrower paying back the loan timely.\n",
        "    Your task is to analyze as per the given instructions - the borrower features provided to the model, the model's prediction and the SHAP values generated by the model; and \\\n",
        "    provide a coherent and non-technical explanation on how the provided features influence the model's prediction.\\n\"\"\"\n",
        "\n",
        "      pred_features_intro = \"\"\"\\nPrediction, features and SHAP values input:\\n\"\"\"\n",
        "      pred_string = f\"\"\"- Prediction for the instance: {round(prediction_proba * 100, 2)}% probability of paying back the loan timely.\\n\"\"\"\n",
        "      features_string = f\"\"\"- Features values and SHAP values of the instance:\\n{features_shaps}\\n\"\"\"\n",
        "      addr_state = get_addr_state(idx = idx)\n",
        "\n",
        "      instructions = f\"\"\"Instructions:\n",
        "    Rank the features based on the magnitude of SHAP values from highest to lowest, regardless of the direction, and pick the top 10 features.\n",
        "\n",
        "    For each feature in the top 10 features:\n",
        "    - Retrieve the feature definition for the feature.\n",
        "    - Utilize the definition to find how the feature influences loan eligibility.\n",
        "    - Utilize the feature's provided SHAP value to determine it's importance for decision making \\\n",
        "    Make sure to interpret the magnitude and direction of the SHAP value correctly.\n",
        "    - Determine how the feature and it's value contribute to the prediction, such as whether a higher value increases the loan repayment probability or decreases it.\\\n",
        "    Make sure you are considering the general context as well as the context of the particular borrower.\n",
        "    - Output the following two sections for each feature before moving to another feature:\n",
        "    Section 1- Feature Definition:\n",
        "    - The title of the feature\n",
        "    - The value of this feature for the borrower\n",
        "    - A brief explanation of what this feature represents, and how it influences the loan repayment probability generally as well as specifically for the borrower.\n",
        "    - Is there a median value for this feature available for the state of {addr_state}? \\\n",
        "    If yes, how does the borrower's value for this feature compare to the state median for this feature? If no median value is available then do note compare.\n",
        "    The length of this section should be between 2 to 4 sentences.\n",
        "    Section 2- SHAP Context:\n",
        "    - The SHAP value for the feature\n",
        "    - A brief explanation of the feature's influence on the model's prediction for this particular borrower as represented by the corresponding SHAP value.\n",
        "    Make sure to interpret the magnitude and direction of the SHAP value correctly. The length of this section should be between 1 to 3 sentences.\n",
        "\n",
        "    Once all features are covered, conclude with a brief summary of how the top 10 influencing features affect the decision and contribute to the model's prediction. This summary should be under 5 sentences.\\n\"\"\"\n",
        "\n",
        "      context = f\"\"\"Context:\\n{context}\\nOutput:\\n\"\"\"\n",
        "\n",
        "      prompt = system_prompt + instructions + pred_features_intro + pred_string + features_string + context\n",
        "      return prompt\n",
        "\n",
        "    #Set up huggingface pipeline\n",
        "    temperature = 0.35\n",
        "    max_length = 7000\n",
        "\n",
        "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length, truncation = True)\n",
        "\n",
        "    llm_agent = HuggingFacePipeline(\n",
        "        pipeline=pipe,\n",
        "        model_kwargs={\"temperature\": temperature, \"max_length\": max_length},\n",
        "    )\n",
        "\n",
        "    def rag_pipeline(idx = 0):\n",
        "      context = generate_context(idx = idx)\n",
        "      prompt = custom_prompt(context = context, idx = idx)\n",
        "      results = llm_agent.invoke(prompt)\n",
        "      return results\n",
        "\n",
        "    def rag_generate(idx = 0):\n",
        "      result = rag_pipeline(idx = idx)\n",
        "      result_index = result.find(\"Output:\\n\")\n",
        "      filter_result = result[result_index+8:]\n",
        "      return filter_result\n",
        "\n",
        "    full_result = rag_generate(idx = idx)\n",
        "    st.write(f\"**Explanation:**\\n\")\n",
        "    st.text(f\"{full_result}\")\n",
        "\n",
        "\n",
        "\n",
        "    def top_shaps_plot(shap_df, idx):\n",
        "      _,shaps = get_features_shaps(shap_row = shap_df, idx = idx)\n",
        "      shaps[\"Effect\"] = shaps[\"SHAP value\"].apply(lambda x: -1 if x < 0 else 1)\n",
        "\n",
        "      #Plot\n",
        "      fig, ax = plt.subplots(figsize=(6, 4))\n",
        "      ax.barh(\n",
        "          y=shaps[\"Feature\"],\n",
        "          width=shaps[\"SHAP value\"],\n",
        "          color=shaps[\"Effect\"].apply(lambda x: \"red\" if x == -1 else \"green\")\n",
        "      )\n",
        "      ax.set_title(\"SHAP values for features\")\n",
        "      ax.set_xlabel(\"SHAP Value (Absolute)\")\n",
        "\n",
        "      legend_items = [\n",
        "          matplotlib.lines.Line2D([0], [0], marker='o', color='w',\n",
        "                                  label='Positive Effect', markerfacecolor='green', markersize=10),\n",
        "          matplotlib.lines.Line2D([0], [0], marker='o', color='w',\n",
        "                                  label='Negative Effect', markerfacecolor='red', markersize=10),\n",
        "      ]\n",
        "      ax.legend(handles=legend_items)\n",
        "\n",
        "      st.write(\"**SHAP value plot:**\\n\")\n",
        "      st.pyplot(fig)\n",
        "\n",
        "    top_shaps_plot(shap_df = shap_row_df, idx = idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_-l1LafMAY6",
        "outputId": "e6f73311-5814-4d51-e10d-ec62ab76ac65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35.229.43.146"
          ]
        }
      ],
      "source": [
        "#Fetch local ip address\n",
        "!curl ifconfig.me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Host app on local tunnel\n",
        "Enter local ip address as password"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn1932DfMCfR",
        "outputId": "f29d69e8-1d79-4c15-ce87-1568cccbe62d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.229.43.146:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0Kyour url is: https://empty-pets-drive.loca.lt\n",
            "2025-04-05 21:19:22.463 Uncaught exception GET /_stcore/stream (127.0.0.1)\n",
            "HTTPServerRequest(protocol='http', host='empty-pets-drive.loca.lt', method='GET', uri='/_stcore/stream', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/websocket.py\", line 938, in _accept_connection\n",
            "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/browser_websocket_handler.py\", line 177, in open\n",
            "    self._session_id = self._runtime.connect_session(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 397, in connect_session\n",
            "    session_id = self._session_mgr.connect_session(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/websocket_session_manager.py\", line 99, in connect_session\n",
            "    session = AppSession(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 157, in __init__\n",
            "    self.register_file_watchers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 194, in register_file_watchers\n",
            "    self._local_sources_watcher = LocalSourcesWatcher(self._pages_manager)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 65, in __init__\n",
            "    self.update_watched_pages()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 77, in update_watched_pages\n",
            "    self._register_watcher(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 136, in _register_watcher\n",
            "    watcher=PathWatcher(filepath, self.on_file_changed),\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 107, in __init__\n",
            "    path_watcher.watch_path(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 185, in watch_path\n",
            "    folder_handler.watch = self._observer.schedule(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/api.py\", line 312, in schedule\n",
            "    emitter.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/utils/__init__.py\", line 75, in start\n",
            "    self.on_thread_start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify.py\", line 119, in on_thread_start\n",
            "    self._inotify = InotifyBuffer(path, recursive=self.watch.is_recursive, event_mask=event_mask)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_buffer.py\", line 30, in __init__\n",
            "    self._inotify = Inotify(path, recursive=recursive, event_mask=event_mask)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 185, in __init__\n",
            "    self._add_dir_watch(path, event_mask, recursive=recursive)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 411, in _add_dir_watch\n",
            "    self._add_watch(full_path, mask)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 424, in _add_watch\n",
            "    Inotify._raise_error()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 441, in _raise_error\n",
            "    raise OSError(err, os.strerror(err))\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Exception ignored in: <function AppSession.__del__ at 0x783b9bb95120>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 177, in __del__\n",
            "    self.shutdown()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 254, in shutdown\n",
            "    self.request_script_stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 426, in request_script_stop\n",
            "    if self._scriptrunner is not None:\n",
            "       ^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'AppSession' object has no attribute '_scriptrunner'\n",
            "/content/app.py:13: LangChainDeprecationWarning: Importing Pinecone from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import Pinecone\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import Pinecone\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import Pinecone as PineconeStore\n",
            "2025-04-05 21:19:28.969039: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-05 21:19:28.987804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743887969.015704   24298 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743887969.024041   24298 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-05 21:19:29.051874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
            "  warnings.warn(\n",
            "2025-04-05 21:19:34.013 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.87s/it]\n",
            "Device set to use cuda:0\n",
            "2025-04-05 21:21:13.997 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
